{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "177ba39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Madesh\\Desktop\\Data Science Project\\Noisy_Speech_Recognition\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3050\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import librosa\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"Running on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b696473d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 28539\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = \"../data/clean/LibriSpeech/train-clean-100\"\n",
    "\n",
    "data = []\n",
    "\n",
    "for root, dirs, files in os.walk(BASE_PATH):\n",
    "    for file in files:\n",
    "        if file.endswith(\".trans.txt\"):\n",
    "            trans_path = os.path.join(root, file)\n",
    "\n",
    "            with open(trans_path, \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(\" \", 1)\n",
    "                file_id = parts[0]\n",
    "                text = parts[1].lower()\n",
    "                audio_path = os.path.join(root, file_id + \".flac\")\n",
    "\n",
    "                if os.path.exists(audio_path):\n",
    "                    data.append({\n",
    "                        \"audio_path\": audio_path,\n",
    "                        \"text\": text\n",
    "                    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Total samples:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28207bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 5000\n"
     ]
    }
   ],
   "source": [
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z ']\", \"\", text)\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(normalize_text)\n",
    "\n",
    "df_train = df.sample(5000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"Training samples:\", len(df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af47f70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise files: 930\n"
     ]
    }
   ],
   "source": [
    "TARGET_SR = 16000\n",
    "\n",
    "def load_audio(path):\n",
    "    y, sr = librosa.load(path, sr=None)\n",
    "\n",
    "    if sr != TARGET_SR:\n",
    "        y = librosa.resample(y, orig_sr=sr, target_sr=TARGET_SR)\n",
    "\n",
    "    return y.astype(\"float32\")\n",
    "\n",
    "noise_files = []\n",
    "noise_root = \"../data/musan/noise\"\n",
    "\n",
    "for root, dirs, files in os.walk(noise_root):\n",
    "    for file in files:\n",
    "        if file.endswith(\".wav\"):\n",
    "            noise_files.append(os.path.join(root, file))\n",
    "\n",
    "print(\"Noise files:\", len(noise_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff758c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(clean, noise, snr_db):\n",
    "    if len(noise) < len(clean):\n",
    "        repeat = int(np.ceil(len(clean) / len(noise)))\n",
    "        noise = np.tile(noise, repeat)\n",
    "\n",
    "    noise = noise[:len(clean)]\n",
    "\n",
    "    clean_power = np.mean(clean ** 2)\n",
    "    noise_power = np.mean(noise ** 2)\n",
    "\n",
    "    snr = 10 ** (snr_db / 10)\n",
    "    scale = np.sqrt(clean_power / (snr * noise_power))\n",
    "\n",
    "    noisy = clean + scale * noise\n",
    "    return noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "365c757c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    }
   ],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(\n",
    "    \"facebook/wav2vec2-base-960h\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4800ed5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [02:53<00:00, 28.82 examples/s]\n"
     ]
    }
   ],
   "source": [
    "TRAIN_MODE = \"clean\"   # first phase\n",
    "\n",
    "def prepare_dataset(example):\n",
    "\n",
    "    audio = load_audio(example[\"audio_path\"])\n",
    "\n",
    "    if TRAIN_MODE == \"noisy\":\n",
    "        noise_audio = load_audio(random.choice(noise_files))\n",
    "        snr_db = np.random.uniform(0, 20)\n",
    "        audio = add_noise(audio, noise_audio, snr_db)\n",
    "\n",
    "    inputs = processor(audio, sampling_rate=16000)\n",
    "\n",
    "    labels = processor(text=example[\"clean_text\"]).input_ids\n",
    "\n",
    "    example[\"input_values\"] = inputs.input_values[0]\n",
    "    example[\"labels\"] = labels\n",
    "\n",
    "    return example\n",
    "\n",
    "dataset = Dataset.from_pandas(df_train)\n",
    "\n",
    "dataset = dataset.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c178b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 212/212 [00:00<00:00, 478.02it/s, Materializing param=wav2vec2.feature_projection.projection.weight]                         \n",
      "\u001b[1mWav2Vec2ForCTC LOAD REPORT\u001b[0m from: facebook/wav2vec2-base-960h\n",
      "Key                        | Status  | \n",
      "---------------------------+---------+-\n",
      "wav2vec2.masked_spec_embed | MISSING | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForCTC(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2GroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2Encoder(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (lm_head): Linear(in_features=768, out_features=32, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-base-960h\"\n",
    ")\n",
    "\n",
    "for param in model.wav2vec2.feature_extractor.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c42f0247",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollatorCTCWithPadding:\n",
    "    def __init__(self, processor, padding=True):\n",
    "        self.processor = processor\n",
    "        self.padding = padding\n",
    "\n",
    "    def __call__(self, features):\n",
    "\n",
    "        input_features = [{\"input_values\": f[\"input_values\"]} for f in features]\n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            label_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"]\n",
    "        labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37f9d2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "\n",
    "    pred_ids = np.argmax(pred.predictions, axis=-1)\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "\n",
    "    label_ids = pred.label_ids\n",
    "    if isinstance(label_ids, torch.Tensor):\n",
    "        label_ids = label_ids.cpu().numpy()\n",
    "\n",
    "    label_ids = label_ids.copy()\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    label_str = processor.batch_decode(label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer, \"cer\": cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5f67f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    num_train_epochs=5,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    learning_rate=3e-6,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    gradient_checkpointing=True,\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e02ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(mode):\n",
    "\n",
    "    print(f\"\\n========== Training {mode.upper()} Model ==========\\n\")\n",
    "\n",
    "    global TRAIN_MODE\n",
    "    TRAIN_MODE = mode\n",
    "\n",
    "    dataset = Dataset.from_pandas(df_train)\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        prepare_dataset,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "\n",
    "    dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    eval_dataset = dataset[\"test\"]\n",
    "\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\n",
    "        \"facebook/wav2vec2-base-960h\"\n",
    "    )\n",
    "\n",
    "\n",
    "    for param in model.wav2vec2.feature_extractor.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    results = trainer.evaluate()\n",
    "\n",
    "    print(f\"\\nResults for {mode.upper()} model:\")\n",
    "    print(results)\n",
    "\n",
    "    return results,trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "baa84514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Training CLEAN Model ==========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [04:53<00:00, 17.02 examples/s]  \n",
      "Loading weights: 100%|██████████| 212/212 [00:00<00:00, 268.01it/s, Materializing param=wav2vec2.feature_projection.projection.weight]                         \n",
      "\u001b[1mWav2Vec2ForCTC LOAD REPORT\u001b[0m from: facebook/wav2vec2-base-960h\n",
      "Key                        | Status  | \n",
      "---------------------------+---------+-\n",
      "wav2vec2.masked_spec_embed | MISSING | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='238' max='2815' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 238/2815 36:50 < 6:42:18, 0.11 it/s, Epoch 0.42/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Cer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>36667.442500</td>\n",
       "      <td>19744.382812</td>\n",
       "      <td>1.001798</td>\n",
       "      <td>0.956847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>29003.135000</td>\n",
       "      <td>9653.660156</td>\n",
       "      <td>1.001044</td>\n",
       "      <td>0.956872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>13693.398750</td>\n",
       "      <td>2814.905029</td>\n",
       "      <td>0.981321</td>\n",
       "      <td>0.399521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>9172.331250</td>\n",
       "      <td>1457.816284</td>\n",
       "      <td>0.999362</td>\n",
       "      <td>0.573854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m clean_results, clean_trainer  = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mclean\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(mode)\u001b[39m\n\u001b[32m     28\u001b[39m model.to(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m trainer = Trainer(\n\u001b[32m     31\u001b[39m     model=model,\n\u001b[32m     32\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     36\u001b[39m     compute_metrics=compute_metrics,\n\u001b[32m     37\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m results = trainer.evaluate()\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mResults for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode.upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m model:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Madesh\\Desktop\\Data Science Project\\Noisy_Speech_Recognition\\.venv\\Lib\\site-packages\\transformers\\trainer.py:1412\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   1410\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   1411\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1412\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1413\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1415\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1416\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1417\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Madesh\\Desktop\\Data Science Project\\Noisy_Speech_Recognition\\.venv\\Lib\\site-packages\\transformers\\trainer.py:1747\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   1741\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sync_context():\n\u001b[32m   1742\u001b[39m     tr_loss_step = \u001b[38;5;28mself\u001b[39m.training_step(model, inputs, num_items_in_batch)\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1745\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   1746\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   1748\u001b[39m ):\n\u001b[32m   1749\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   1750\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n\u001b[32m   1751\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "clean_results, clean_trainer  = train_model(\"clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7779495",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m pred = \u001b[43mclean_trainer\u001b[49m.predict(clean_trainer.eval_dataset)\n\u001b[32m      3\u001b[39m pred_ids = np.argmax(pred.predictions, axis=-\u001b[32m1\u001b[39m)\n\u001b[32m      5\u001b[39m pred_str = processor.batch_decode(pred_ids)\n",
      "\u001b[31mNameError\u001b[39m: name 'clean_trainer' is not defined"
     ]
    }
   ],
   "source": [
    "pred = clean_trainer.predict(clean_trainer.eval_dataset)\n",
    "\n",
    "pred_ids = np.argmax(pred.predictions, axis=-1)\n",
    "\n",
    "pred_str = processor.batch_decode(pred_ids)\n",
    "label_ids = pred.label_ids.copy()\n",
    "label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "label_str = processor.batch_decode(label_ids, group_tokens=False)\n",
    "\n",
    "print(\"PREDICTIONS:\")\n",
    "print(pred_str[:5])\n",
    "print(\"LABELS:\")\n",
    "print(label_str[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e919584",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_results = train_model(\"noisy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514c4326",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Clean model:\", clean_results)\n",
    "print(\"Noisy model:\", noisy_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
