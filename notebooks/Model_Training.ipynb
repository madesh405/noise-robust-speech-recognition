{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "177ba39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Madesh\\Desktop\\Data Science Project\\Noisy_Speech_Recognition\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3050\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import librosa\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"Running on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b696473d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 28539\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = \"../data/clean/LibriSpeech/train-clean-100\"\n",
    "\n",
    "data = []\n",
    "\n",
    "for root, dirs, files in os.walk(BASE_PATH):\n",
    "    for file in files:\n",
    "        if file.endswith(\".trans.txt\"):\n",
    "            trans_path = os.path.join(root, file)\n",
    "\n",
    "            with open(trans_path, \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(\" \", 1)\n",
    "                file_id = parts[0]\n",
    "                text = parts[1].lower()\n",
    "                audio_path = os.path.join(root, file_id + \".flac\")\n",
    "\n",
    "                if os.path.exists(audio_path):\n",
    "                    data.append({\n",
    "                        \"audio_path\": audio_path,\n",
    "                        \"text\": text\n",
    "                    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Total samples:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28207bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 5000\n"
     ]
    }
   ],
   "source": [
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z ']\", \"\", text)\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(normalize_text)\n",
    "\n",
    "df_train = df.sample(5000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"Training samples:\", len(df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af47f70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise files: 930\n"
     ]
    }
   ],
   "source": [
    "TARGET_SR = 16000\n",
    "\n",
    "def load_audio(path):\n",
    "    y, sr = librosa.load(path, sr=None)\n",
    "\n",
    "    if sr != TARGET_SR:\n",
    "        y = librosa.resample(y, orig_sr=sr, target_sr=TARGET_SR)\n",
    "\n",
    "    return y.astype(\"float32\")\n",
    "\n",
    "noise_files = []\n",
    "noise_root = \"../data/musan/noise\"\n",
    "\n",
    "for root, dirs, files in os.walk(noise_root):\n",
    "    for file in files:\n",
    "        if file.endswith(\".wav\"):\n",
    "            noise_files.append(os.path.join(root, file))\n",
    "\n",
    "print(\"Noise files:\", len(noise_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff758c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(clean, noise, snr_db):\n",
    "    if len(noise) < len(clean):\n",
    "        repeat = int(np.ceil(len(clean) / len(noise)))\n",
    "        noise = np.tile(noise, repeat)\n",
    "\n",
    "    noise = noise[:len(clean)]\n",
    "\n",
    "    clean_power = np.mean(clean ** 2)\n",
    "    noise_power = np.mean(noise ** 2)\n",
    "\n",
    "    snr = 10 ** (snr_db / 10)\n",
    "    scale = np.sqrt(clean_power / (snr * noise_power))\n",
    "\n",
    "    noisy = clean + scale * noise\n",
    "    return noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "365c757c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    }
   ],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(\n",
    "    \"facebook/wav2vec2-base-960h\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4800ed5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:56<00:00, 88.88 examples/s] \n"
     ]
    }
   ],
   "source": [
    "TRAIN_MODE = \"clean\"   # first phase\n",
    "\n",
    "def prepare_dataset(example):\n",
    "\n",
    "    audio = load_audio(example[\"audio_path\"])\n",
    "\n",
    "    if TRAIN_MODE == \"noisy\":\n",
    "        noise_audio = load_audio(random.choice(noise_files))\n",
    "        snr_db = np.random.uniform(0, 20)\n",
    "        audio = add_noise(audio, noise_audio, snr_db)\n",
    "\n",
    "    inputs = processor(audio, sampling_rate=16000)\n",
    "\n",
    "    labels = processor(text=example[\"clean_text\"].upper()).input_ids\n",
    "\n",
    "    example[\"input_values\"] = inputs.input_values[0]\n",
    "    example[\"labels\"] = labels\n",
    "\n",
    "    return example\n",
    "\n",
    "dataset = Dataset.from_pandas(df_train)\n",
    "\n",
    "dataset = dataset.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c178b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 212/212 [00:00<00:00, 536.01it/s, Materializing param=wav2vec2.feature_projection.projection.weight]                         \n",
      "\u001b[1mWav2Vec2ForCTC LOAD REPORT\u001b[0m from: facebook/wav2vec2-base-960h\n",
      "Key                        | Status  | \n",
      "---------------------------+---------+-\n",
      "wav2vec2.masked_spec_embed | MISSING | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForCTC(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2GroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2Encoder(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (lm_head): Linear(in_features=768, out_features=32, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-base-960h\"\n",
    ")\n",
    "\n",
    "for param in model.wav2vec2.feature_extractor.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c42f0247",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollatorCTCWithPadding:\n",
    "    def __init__(self, processor, padding=True):\n",
    "        self.processor = processor\n",
    "        self.padding = padding\n",
    "\n",
    "    def __call__(self, features):\n",
    "\n",
    "        input_features = [{\"input_values\": f[\"input_values\"]} for f in features]\n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            label_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"]\n",
    "        labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37f9d2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = np.argmax(pred.predictions, axis=-1)\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids, group_tokens=True)\n",
    "\n",
    "    label_ids = pred.label_ids\n",
    "    if isinstance(label_ids, torch.Tensor):\n",
    "        label_ids = label_ids.cpu().numpy()\n",
    "\n",
    "    label_ids = label_ids.copy()\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    label_str = processor.batch_decode(label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer, \"cer\": cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b5f67f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    num_train_epochs=5,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    learning_rate=3e-5,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    gradient_checkpointing=True,\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92e02ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(mode):\n",
    "\n",
    "    print(f\"\\n========== Training {mode.upper()} Model ==========\\n\")\n",
    "\n",
    "    global TRAIN_MODE\n",
    "    TRAIN_MODE = mode\n",
    "\n",
    "    dataset = Dataset.from_pandas(df_train)\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        prepare_dataset,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "\n",
    "    dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    eval_dataset = dataset[\"test\"]\n",
    "\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\n",
    "        \"facebook/wav2vec2-base-960h\"\n",
    "    )\n",
    "\n",
    "\n",
    "    # for param in model.wav2vec2.feature_extractor.parameters():\n",
    "    #     param.requires_grad = False\n",
    "\n",
    "    model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    results = trainer.evaluate()\n",
    "\n",
    "    print(f\"\\nResults for {mode.upper()} model:\")\n",
    "    print(results)\n",
    "\n",
    "    return results, trainer, eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "baa84514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Training CLEAN Model ==========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:56<00:00, 88.92 examples/s] \n",
      "Loading weights: 100%|██████████| 212/212 [00:00<00:00, 322.68it/s, Materializing param=wav2vec2.feature_projection.projection.weight]                         \n",
      "\u001b[1mWav2Vec2ForCTC LOAD REPORT\u001b[0m from: facebook/wav2vec2-base-960h\n",
      "Key                        | Status  | \n",
      "---------------------------+---------+-\n",
      "wav2vec2.masked_spec_embed | MISSING | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2815' max='2815' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2815/2815 1:26:15, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Cer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>834.336094</td>\n",
       "      <td>406.376312</td>\n",
       "      <td>0.995984</td>\n",
       "      <td>0.955752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>725.766406</td>\n",
       "      <td>365.102814</td>\n",
       "      <td>0.990223</td>\n",
       "      <td>0.902141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>798.126875</td>\n",
       "      <td>423.395752</td>\n",
       "      <td>0.985393</td>\n",
       "      <td>0.938645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>706.690469</td>\n",
       "      <td>412.927399</td>\n",
       "      <td>0.982890</td>\n",
       "      <td>0.948729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>719.989844</td>\n",
       "      <td>442.746307</td>\n",
       "      <td>0.983123</td>\n",
       "      <td>0.948652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|██████████| 1/1 [00:03<00:00,  3.84s/it]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:02<00:00,  2.36s/it]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  2.81it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  1.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:55]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for CLEAN model:\n",
      "{'eval_loss': 412.9273986816406, 'eval_wer': 0.9828900657626725, 'eval_cer': 0.9487292942753286, 'eval_runtime': 56.9814, 'eval_samples_per_second': 8.775, 'eval_steps_per_second': 2.194, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "clean_results, clean_trainer, clean_eval = train_model(\"clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b3c2e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      "<unk><unk><unk><unk>\n",
      "\n",
      "Label:\n",
      "<unk><unk><unk><unk><unk><unk><unk> <unk><unk><unk><unk><unk><unk><unk> <unk><unk> <unk><unk><unk> <unk><unk> <unk><unk><unk> <unk><unk><unk><unk><unk><unk> <unk><unk> <unk><unk> <unk><unk><unk><unk> <unk><unk><unk><unk><unk> <unk><unk> <unk><unk><unk> <unk><unk><unk><unk><unk> <unk><unk><unk><unk><unk><unk><unk> <unk><unk><unk><unk><unk> <unk><unk><unk><unk> <unk><unk><unk> <unk><unk><unk><unk><unk><unk><unk><unk> <unk><unk><unk><unk><unk><unk><unk><unk> <unk><unk> <unk><unk><unk> <unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk> <unk><unk><unk><unk><unk> <unk><unk> <unk><unk><unk>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Take one sample from validation set\n",
    "sample = clean_eval[0]\n",
    "\n",
    "model = clean_trainer.model\n",
    "model.eval()\n",
    "\n",
    "# Prepare input tensor\n",
    "input_tensor = torch.tensor(sample[\"input_values\"]).unsqueeze(0).to(model.device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tensor).logits\n",
    "\n",
    "# Get predicted token IDs\n",
    "pred_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# Decode prediction and label\n",
    "prediction = processor.batch_decode(pred_ids, group_tokens=True)[0]\n",
    "label = processor.batch_decode(\n",
    "    [sample[\"labels\"]],\n",
    "    group_tokens=False\n",
    ")[0]\n",
    "\n",
    "print(\"Prediction:\")\n",
    "print(prediction)\n",
    "print(\"\\nLabel:\")\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e919584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Training NOISY Model ==========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  20%|█▉        | 999/5000 [00:34<02:18, 28.98 examples/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m noisy_results, noisy_trainer, noisy_eval = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnoisy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(mode)\u001b[39m\n\u001b[32m      6\u001b[39m TRAIN_MODE = mode\n\u001b[32m      8\u001b[39m dataset = Dataset.from_pandas(df_train)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m dataset = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprepare_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumn_names\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m dataset = dataset.train_test_split(test_size=\u001b[32m0.1\u001b[39m)\n\u001b[32m     17\u001b[39m train_dataset = dataset[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Madesh\\Desktop\\Data Science Project\\Noisy_Speech_Recognition\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:562\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    555\u001b[39m self_format = {\n\u001b[32m    556\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    557\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    558\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    559\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    560\u001b[39m }\n\u001b[32m    561\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    563\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Madesh\\Desktop\\Data Science Project\\Noisy_Speech_Recognition\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:3343\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3341\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3342\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m unprocessed_kwargs \u001b[38;5;129;01min\u001b[39;00m unprocessed_kwargs_per_job:\n\u001b[32m-> \u001b[39m\u001b[32m3343\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43munprocessed_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3344\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcheck_if_shard_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3346\u001b[39m \u001b[38;5;66;03m# Avoids PermissionError on Windows (the error: https://github.com/huggingface/datasets/actions/runs/4026734820/jobs/6921621805)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Madesh\\Desktop\\Data Science Project\\Noisy_Speech_Recognition\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:3691\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[39m\n\u001b[32m   3689\u001b[39m         writer.write_row(example.to_arrow())\n\u001b[32m   3690\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3691\u001b[39m         \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3692\u001b[39m num_examples_progress_update += \u001b[32m1\u001b[39m\n\u001b[32m   3693\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m time.time() > _time + config.PBAR_REFRESH_TIME_INTERVAL:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Madesh\\Desktop\\Data Science Project\\Noisy_Speech_Recognition\\.venv\\Lib\\site-packages\\datasets\\arrow_writer.py:598\u001b[39m, in \u001b[36mArrowWriter.write\u001b[39m\u001b[34m(self, example, writer_batch_size)\u001b[39m\n\u001b[32m    596\u001b[39m     writer_batch_size = \u001b[38;5;28mself\u001b[39m.writer_batch_size\n\u001b[32m    597\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m writer_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.current_examples) >= writer_batch_size:\n\u001b[32m--> \u001b[39m\u001b[32m598\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite_examples_on_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Madesh\\Desktop\\Data Science Project\\Noisy_Speech_Recognition\\.venv\\Lib\\site-packages\\datasets\\arrow_writer.py:571\u001b[39m, in \u001b[36mArrowWriter.write_examples_on_file\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    566\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    567\u001b[39m         batch_examples[col] = [\n\u001b[32m    568\u001b[39m             row[\u001b[32m0\u001b[39m][col].to_pylist()[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(row[\u001b[32m0\u001b[39m][col], (pa.Array, pa.ChunkedArray)) \u001b[38;5;28;01melse\u001b[39;00m row[\u001b[32m0\u001b[39m][col]\n\u001b[32m    569\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_examples\n\u001b[32m    570\u001b[39m         ]\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_examples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_examples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[38;5;28mself\u001b[39m.current_examples = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Madesh\\Desktop\\Data Science Project\\Noisy_Speech_Recognition\\.venv\\Lib\\site-packages\\datasets\\arrow_writer.py:658\u001b[39m, in \u001b[36mArrowWriter.write_batch\u001b[39m\u001b[34m(self, batch_examples, writer_batch_size, try_original_type)\u001b[39m\n\u001b[32m    656\u001b[39m         typed_sequence = OptimizedTypedSequence(col_values, \u001b[38;5;28mtype\u001b[39m=col_type, try_type=col_try_type, col=col)\n\u001b[32m    657\u001b[39m         arrays.append(pa.array(typed_sequence))\n\u001b[32m--> \u001b[39m\u001b[32m658\u001b[39m         inferred_features[col] = \u001b[43mtyped_sequence\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_inferred_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    659\u001b[39m schema = inferred_features.arrow_schema \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pa_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.schema\n\u001b[32m    660\u001b[39m pa_table = pa.Table.from_arrays(arrays, schema=schema)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Madesh\\Desktop\\Data Science Project\\Noisy_Speech_Recognition\\.venv\\Lib\\site-packages\\datasets\\arrow_writer.py:221\u001b[39m, in \u001b[36mTypedSequence.get_inferred_type\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# used to get back the inferred type after __arrow_array__() is called once\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;28mself\u001b[39m._inferred_type = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_inferred_type\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> FeatureType:\n\u001b[32m    222\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the inferred feature type.\u001b[39;00m\n\u001b[32m    223\u001b[39m \u001b[33;03m    This is done by converting the sequence to an Arrow array, and getting the corresponding\u001b[39;00m\n\u001b[32m    224\u001b[39m \u001b[33;03m    feature type.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    230\u001b[39m \u001b[33;03m        FeatureType: inferred feature type of the sequence.\u001b[39;00m\n\u001b[32m    231\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    232\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inferred_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "noisy_results, noisy_trainer, noisy_eval = train_model(\"noisy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514c4326",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Clean model:\", clean_results)\n",
    "print(\"Noisy model:\", noisy_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
