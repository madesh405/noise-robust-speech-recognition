{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "177ba39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Madesh\\Desktop\\Data Science Project\\Noisy_Speech_Recognition\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3050\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import librosa\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"Running on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b696473d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 28539\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = \"../data/clean/LibriSpeech/train-clean-100\"\n",
    "\n",
    "data = []\n",
    "\n",
    "for root, dirs, files in os.walk(BASE_PATH):\n",
    "    for file in files:\n",
    "        if file.endswith(\".trans.txt\"):\n",
    "            trans_path = os.path.join(root, file)\n",
    "\n",
    "            with open(trans_path, \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(\" \", 1)\n",
    "                file_id = parts[0]\n",
    "                text = parts[1].lower()\n",
    "                audio_path = os.path.join(root, file_id + \".flac\")\n",
    "\n",
    "                if os.path.exists(audio_path):\n",
    "                    data.append({\n",
    "                        \"audio_path\": audio_path,\n",
    "                        \"text\": text\n",
    "                    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Total samples:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28207bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 5000\n"
     ]
    }
   ],
   "source": [
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z ']\", \"\", text)\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(normalize_text)\n",
    "\n",
    "df_train = df.sample(5000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"Training samples:\", len(df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af47f70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise files: 930\n"
     ]
    }
   ],
   "source": [
    "TARGET_SR = 16000\n",
    "\n",
    "def load_audio(path):\n",
    "    y, sr = librosa.load(path, sr=None)\n",
    "\n",
    "    if sr != TARGET_SR:\n",
    "        y = librosa.resample(y, orig_sr=sr, target_sr=TARGET_SR)\n",
    "\n",
    "    return y.astype(\"float32\")\n",
    "\n",
    "noise_files = []\n",
    "noise_root = \"../data/musan/noise\"\n",
    "\n",
    "for root, dirs, files in os.walk(noise_root):\n",
    "    for file in files:\n",
    "        if file.endswith(\".wav\"):\n",
    "            noise_files.append(os.path.join(root, file))\n",
    "\n",
    "print(\"Noise files:\", len(noise_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff758c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(clean, noise, snr_db):\n",
    "    if len(noise) < len(clean):\n",
    "        repeat = int(np.ceil(len(clean) / len(noise)))\n",
    "        noise = np.tile(noise, repeat)\n",
    "\n",
    "    noise = noise[:len(clean)]\n",
    "\n",
    "    clean_power = np.mean(clean ** 2)\n",
    "    noise_power = np.mean(noise ** 2)\n",
    "\n",
    "    snr = 10 ** (snr_db / 10)\n",
    "    scale = np.sqrt(clean_power / (snr * noise_power))\n",
    "\n",
    "    noisy = clean + scale * noise\n",
    "    return noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "365c757c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    }
   ],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(\n",
    "    \"facebook/wav2vec2-base-960h\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4800ed5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  29%|██▉       | 1467/5000 [00:15<00:28, 126.08 examples/s]"
     ]
    }
   ],
   "source": [
    "TRAIN_MODE = \"clean\"   # first phase\n",
    "\n",
    "def prepare_dataset(example):\n",
    "\n",
    "    audio = load_audio(example[\"audio_path\"])\n",
    "\n",
    "    if TRAIN_MODE == \"noisy\":\n",
    "        noise_audio = load_audio(random.choice(noise_files))\n",
    "        snr_db = np.random.uniform(0, 20)\n",
    "        audio = add_noise(audio, noise_audio, snr_db)\n",
    "\n",
    "    inputs = processor(audio, sampling_rate=16000)\n",
    "\n",
    "    labels = processor(text=example[\"clean_text\"].upper()).input_ids\n",
    "\n",
    "    example[\"input_values\"] = inputs.input_values[0]\n",
    "    example[\"labels\"] = labels\n",
    "\n",
    "    return example\n",
    "\n",
    "dataset = Dataset.from_pandas(df_train)\n",
    "\n",
    "dataset = dataset.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c178b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-base-960h\"\n",
    ")\n",
    "\n",
    "for param in model.wav2vec2.feature_extractor.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42f0247",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollatorCTCWithPadding:\n",
    "    def __init__(self, processor, padding=True):\n",
    "        self.processor = processor\n",
    "        self.padding = padding\n",
    "\n",
    "    def __call__(self, features):\n",
    "\n",
    "        input_features = [{\"input_values\": f[\"input_values\"]} for f in features]\n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            label_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"]\n",
    "        labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f9d2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = np.argmax(pred.predictions, axis=-1)\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids, group_tokens=True)\n",
    "\n",
    "    label_ids = pred.label_ids\n",
    "    if isinstance(label_ids, torch.Tensor):\n",
    "        label_ids = label_ids.cpu().numpy()\n",
    "\n",
    "    label_ids = label_ids.copy()\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    label_str = processor.batch_decode(label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer, \"cer\": cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5f67f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    num_train_epochs=5,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    learning_rate=3e-5,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    gradient_checkpointing=True,\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ad901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# FIX PROJECT ROOT PATH\n",
    "# ==============================\n",
    "import os\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "MODELS_DIR = os.path.join(PROJECT_ROOT, \"models\")\n",
    "RESULTS_DIR = os.path.join(PROJECT_ROOT, \"results\")\n",
    "\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Saving to:\", PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e02ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from turtle import mode\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "def train_model(mode):\n",
    "\n",
    "    print(f\"\\n========== Training {mode.upper()} Model ==========\\n\")\n",
    "\n",
    "    global TRAIN_MODE\n",
    "    TRAIN_MODE = mode\n",
    "\n",
    "    # -------- Dataset --------\n",
    "    dataset = Dataset.from_pandas(df_train)\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        prepare_dataset,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "\n",
    "    dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    eval_dataset = dataset[\"test\"]\n",
    "\n",
    "    # -------- Model --------\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\n",
    "        \"facebook/wav2vec2-base-960h\"\n",
    "    )\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "\n",
    "    # -------- Create Mode-Specific Folders --------\n",
    "    model_dir = os.path.join(MODELS_DIR, mode)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # -------- Training Arguments (MODE-SPECIFIC) --------\n",
    "    mode_training_args = TrainingArguments(\n",
    "        output_dir=model_dir,        \n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=5,\n",
    "        fp16=True,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    # -------- Trainer --------\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=mode_training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    results = trainer.evaluate()\n",
    "    # Save final trained model separately\n",
    "    final_model_path = os.path.join(model_dir, \"final_model\")\n",
    "    trainer.save_model(final_model_path)\n",
    "\n",
    "    print(f\"\\nResults for {mode.upper()} model:\")\n",
    "    print(results)\n",
    "\n",
    "    # -------- Save Metrics Properly --------\n",
    "    metrics_path = os.path.join(RESULTS_DIR, f\"{mode}_metrics.json\")\n",
    "\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    return results, trainer, eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa84514",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_results, clean_trainer, clean_eval = train_model(\"clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3c2e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Take one sample from validation set\n",
    "sample = clean_eval[0]\n",
    "\n",
    "model = clean_trainer.model\n",
    "model.eval()\n",
    "\n",
    "# Prepare input tensor\n",
    "input_tensor = torch.tensor(sample[\"input_values\"]).unsqueeze(0).to(model.device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tensor).logits\n",
    "\n",
    "# Get predicted token IDs\n",
    "pred_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# Decode prediction and label\n",
    "prediction = processor.batch_decode(pred_ids, group_tokens=True)[0]\n",
    "label = processor.batch_decode(\n",
    "    [sample[\"labels\"]],\n",
    "    group_tokens=False\n",
    ")[0]\n",
    "\n",
    "print(\"Prediction:\")\n",
    "print(prediction)\n",
    "print(\"\\nLabel:\")\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e919584",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_results, noisy_trainer, noisy_eval = train_model(\"noisy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee44a58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Take one sample from noisy validation set\n",
    "sample = noisy_eval[0]\n",
    "\n",
    "model = noisy_trainer.model\n",
    "model.eval()\n",
    "\n",
    "# Prepare input tensor\n",
    "input_tensor = torch.tensor(sample[\"input_values\"]).unsqueeze(0).to(model.device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tensor).logits\n",
    "\n",
    "# Get predicted token IDs\n",
    "pred_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# Decode prediction and label\n",
    "prediction = processor.batch_decode(pred_ids, group_tokens=True)[0]\n",
    "label = processor.batch_decode(\n",
    "    [sample[\"labels\"]],\n",
    "    group_tokens=False\n",
    ")[0]\n",
    "\n",
    "print(\"NOISY MODEL Prediction:\")\n",
    "print(prediction)\n",
    "print(\"\\nActual Label:\")\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514c4326",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Clean model:\", clean_results)\n",
    "print(\"Noisy model:\", noisy_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
