{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "177ba39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Madesh\\Desktop\\Data Science Project\\Noisy_Speech_Recognition\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3050\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import librosa\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"Running on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b696473d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 28539\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = \"../data/clean/LibriSpeech/train-clean-100\"\n",
    "\n",
    "data = []\n",
    "\n",
    "for root, dirs, files in os.walk(BASE_PATH):\n",
    "    for file in files:\n",
    "        if file.endswith(\".trans.txt\"):\n",
    "            trans_path = os.path.join(root, file)\n",
    "\n",
    "            with open(trans_path, \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(\" \", 1)\n",
    "                file_id = parts[0]\n",
    "                text = parts[1].lower()\n",
    "                audio_path = os.path.join(root, file_id + \".flac\")\n",
    "\n",
    "                if os.path.exists(audio_path):\n",
    "                    data.append({\n",
    "                        \"audio_path\": audio_path,\n",
    "                        \"text\": text\n",
    "                    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Total samples:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28207bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 5000\n"
     ]
    }
   ],
   "source": [
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z ']\", \"\", text)\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(normalize_text)\n",
    "\n",
    "df_train = df.sample(5000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"Training samples:\", len(df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af47f70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise files: 930\n"
     ]
    }
   ],
   "source": [
    "TARGET_SR = 16000\n",
    "\n",
    "def load_audio(path):\n",
    "    y, sr = librosa.load(path, sr=None)\n",
    "\n",
    "    if sr != TARGET_SR:\n",
    "        y = librosa.resample(y, orig_sr=sr, target_sr=TARGET_SR)\n",
    "\n",
    "    return y.astype(\"float32\")\n",
    "\n",
    "noise_files = []\n",
    "noise_root = \"../data/musan/noise\"\n",
    "\n",
    "for root, dirs, files in os.walk(noise_root):\n",
    "    for file in files:\n",
    "        if file.endswith(\".wav\"):\n",
    "            noise_files.append(os.path.join(root, file))\n",
    "\n",
    "print(\"Noise files:\", len(noise_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff758c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(clean, noise, snr_db):\n",
    "    if len(noise) < len(clean):\n",
    "        repeat = int(np.ceil(len(clean) / len(noise)))\n",
    "        noise = np.tile(noise, repeat)\n",
    "\n",
    "    noise = noise[:len(clean)]\n",
    "\n",
    "    clean_power = np.mean(clean ** 2)\n",
    "    noise_power = np.mean(noise ** 2)\n",
    "\n",
    "    snr = 10 ** (snr_db / 10)\n",
    "    scale = np.sqrt(clean_power / (snr * noise_power))\n",
    "\n",
    "    noisy = clean + scale * noise\n",
    "    return noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "365c757c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    }
   ],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(\n",
    "    \"facebook/wav2vec2-base-960h\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4800ed5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:48<00:00, 102.64 examples/s]\n"
     ]
    }
   ],
   "source": [
    "TRAIN_MODE = \"clean\"   # first phase\n",
    "\n",
    "def prepare_dataset(example):\n",
    "\n",
    "    audio = load_audio(example[\"audio_path\"])\n",
    "\n",
    "    if TRAIN_MODE == \"noisy\":\n",
    "        noise_audio = load_audio(random.choice(noise_files))\n",
    "        snr_db = np.random.uniform(0, 20)\n",
    "        audio = add_noise(audio, noise_audio, snr_db)\n",
    "\n",
    "    inputs = processor(audio, sampling_rate=16000)\n",
    "\n",
    "    labels = processor(text=example[\"clean_text\"].upper()).input_ids\n",
    "\n",
    "    example[\"input_values\"] = inputs.input_values[0]\n",
    "    example[\"labels\"] = labels\n",
    "\n",
    "    return example\n",
    "\n",
    "dataset = Dataset.from_pandas(df_train)\n",
    "\n",
    "dataset = dataset.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c178b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 212/212 [00:00<00:00, 465.24it/s, Materializing param=wav2vec2.feature_projection.projection.weight]                         \n",
      "\u001b[1mWav2Vec2ForCTC LOAD REPORT\u001b[0m from: facebook/wav2vec2-base-960h\n",
      "Key                        | Status  | \n",
      "---------------------------+---------+-\n",
      "wav2vec2.masked_spec_embed | MISSING | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForCTC(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2GroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2Encoder(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (lm_head): Linear(in_features=768, out_features=32, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-base-960h\"\n",
    ")\n",
    "\n",
    "for param in model.wav2vec2.feature_extractor.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c42f0247",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollatorCTCWithPadding:\n",
    "    def __init__(self, processor, padding=True):\n",
    "        self.processor = processor\n",
    "        self.padding = padding\n",
    "\n",
    "    def __call__(self, features):\n",
    "\n",
    "        input_features = [{\"input_values\": f[\"input_values\"]} for f in features]\n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            label_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"]\n",
    "        labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37f9d2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = np.argmax(pred.predictions, axis=-1)\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids, group_tokens=True)\n",
    "\n",
    "    label_ids = pred.label_ids\n",
    "    if isinstance(label_ids, torch.Tensor):\n",
    "        label_ids = label_ids.cpu().numpy()\n",
    "\n",
    "    label_ids = label_ids.copy()\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    label_str = processor.batch_decode(label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer, \"cer\": cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b5f67f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    num_train_epochs=5,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    learning_rate=3e-5,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    gradient_checkpointing=True,\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22ad901f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to: c:\\Users\\Madesh\\Desktop\\Data Science Project\\Noisy_Speech_Recognition\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# FIX PROJECT ROOT PATH\n",
    "# ==============================\n",
    "import os\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "MODELS_DIR = os.path.join(PROJECT_ROOT, \"models\")\n",
    "RESULTS_DIR = os.path.join(PROJECT_ROOT, \"results\")\n",
    "\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Saving to:\", PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92e02ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from turtle import mode\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "def train_model(mode):\n",
    "\n",
    "    print(f\"\\n========== Training {mode.upper()} Model ==========\\n\")\n",
    "\n",
    "    global TRAIN_MODE\n",
    "    TRAIN_MODE = mode\n",
    "\n",
    "    # -------- Dataset --------\n",
    "    dataset = Dataset.from_pandas(df_train)\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        prepare_dataset,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "\n",
    "    dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    eval_dataset = dataset[\"test\"]\n",
    "\n",
    "    # -------- Model --------\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\n",
    "        \"facebook/wav2vec2-base-960h\"\n",
    "    )\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "\n",
    "    # -------- Create Mode-Specific Folders --------\n",
    "    model_dir = os.path.join(MODELS_DIR, mode)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # -------- Training Arguments (MODE-SPECIFIC) --------\n",
    "    mode_training_args = TrainingArguments(\n",
    "        output_dir=model_dir,        \n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=5,\n",
    "        fp16=True,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    # -------- Trainer --------\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=mode_training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    results = trainer.evaluate()\n",
    "    # Save final trained model separately\n",
    "    final_model_path = os.path.join(model_dir, \"final_model\")\n",
    "    trainer.save_model(final_model_path)\n",
    "\n",
    "    print(f\"\\nResults for {mode.upper()} model:\")\n",
    "    print(results)\n",
    "\n",
    "    # -------- Save Metrics Properly --------\n",
    "    metrics_path = os.path.join(RESULTS_DIR, f\"{mode}_metrics.json\")\n",
    "\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    return results, trainer, eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "baa84514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Training CLEAN Model ==========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:55<00:00, 90.83 examples/s] \n",
      "Loading weights: 100%|██████████| 212/212 [00:00<00:00, 312.23it/s, Materializing param=wav2vec2.feature_projection.projection.weight]                         \n",
      "\u001b[1mWav2Vec2ForCTC LOAD REPORT\u001b[0m from: facebook/wav2vec2-base-960h\n",
      "Key                        | Status  | \n",
      "---------------------------+---------+-\n",
      "wav2vec2.masked_spec_embed | MISSING | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5625' max='5625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5625/5625 1:31:50, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Cer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>73.795076</td>\n",
       "      <td>15.834026</td>\n",
       "      <td>0.025549</td>\n",
       "      <td>0.006117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>54.939135</td>\n",
       "      <td>15.345047</td>\n",
       "      <td>0.024676</td>\n",
       "      <td>0.005854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>46.532101</td>\n",
       "      <td>16.430861</td>\n",
       "      <td>0.023279</td>\n",
       "      <td>0.005821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>41.408823</td>\n",
       "      <td>14.134265</td>\n",
       "      <td>0.020544</td>\n",
       "      <td>0.005076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>35.186330</td>\n",
       "      <td>14.800433</td>\n",
       "      <td>0.019845</td>\n",
       "      <td>0.004845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:07<00:00,  7.59s/it]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  1.32it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:49]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for CLEAN model:\n",
      "{'eval_loss': 14.134264945983887, 'eval_wer': 0.02054356049583891, 'eval_cer': 0.00507558566558139, 'eval_runtime': 51.3497, 'eval_samples_per_second': 9.737, 'eval_steps_per_second': 2.434, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clean_results, clean_trainer, clean_eval = train_model(\"clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b3c2e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      "HERBERT WOUNDED WE ARE AT THE CORRAL BE ON YOUR GUARD DO NOT LEAVE GRANITE HOUSE HAVE THE CONVICTS APPEARED IN THE NEIGHBORHOOD REPLY BY TOP\n",
      "\n",
      "Label:\n",
      "HERBERT WOUNDED WE ARE AT THE CORRAL BE ON YOUR GUARD DO NOT LEAVE GRANITE HOUSE HAVE THE CONVICTS APPEARED IN THE NEIGHBORHOOD REPLY BY TOP\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Take one sample from validation set\n",
    "sample = clean_eval[0]\n",
    "\n",
    "model = clean_trainer.model\n",
    "model.eval()\n",
    "\n",
    "# Prepare input tensor\n",
    "input_tensor = torch.tensor(sample[\"input_values\"]).unsqueeze(0).to(model.device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tensor).logits\n",
    "\n",
    "# Get predicted token IDs\n",
    "pred_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# Decode prediction and label\n",
    "prediction = processor.batch_decode(pred_ids, group_tokens=True)[0]\n",
    "label = processor.batch_decode(\n",
    "    [sample[\"labels\"]],\n",
    "    group_tokens=False\n",
    ")[0]\n",
    "\n",
    "print(\"Prediction:\")\n",
    "print(prediction)\n",
    "print(\"\\nLabel:\")\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e919584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Training NOISY Model ==========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [01:46<00:00, 46.84 examples/s] \n",
      "Loading weights: 100%|██████████| 212/212 [00:00<00:00, 282.29it/s, Materializing param=wav2vec2.feature_projection.projection.weight]                         \n",
      "\u001b[1mWav2Vec2ForCTC LOAD REPORT\u001b[0m from: facebook/wav2vec2-base-960h\n",
      "Key                        | Status  | \n",
      "---------------------------+---------+-\n",
      "wav2vec2.masked_spec_embed | MISSING | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5625' max='5625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5625/5625 2:10:23, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Cer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>198.517139</td>\n",
       "      <td>79.316620</td>\n",
       "      <td>0.068027</td>\n",
       "      <td>0.027449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>139.094819</td>\n",
       "      <td>70.640381</td>\n",
       "      <td>0.062585</td>\n",
       "      <td>0.025174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>112.682076</td>\n",
       "      <td>68.967575</td>\n",
       "      <td>0.056633</td>\n",
       "      <td>0.022514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>98.615569</td>\n",
       "      <td>65.304962</td>\n",
       "      <td>0.053685</td>\n",
       "      <td>0.020506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>85.236437</td>\n",
       "      <td>62.061741</td>\n",
       "      <td>0.051134</td>\n",
       "      <td>0.019940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s]\n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  2.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:49]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|██████████| 1/1 [00:01<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for NOISY model:\n",
      "{'eval_loss': 62.06174087524414, 'eval_wer': 0.051133786848072564, 'eval_cer': 0.019940403080242232, 'eval_runtime': 51.5948, 'eval_samples_per_second': 9.691, 'eval_steps_per_second': 2.423, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "noisy_results, noisy_trainer, noisy_eval = train_model(\"noisy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee44a58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOISY MODEL Prediction:\n",
      "IN THE MEANWHILE ALSO THE BLACK GROUND WAS COVERED WITH IRGE TH GLEN BALS INTERSPRUSED WITH IMMUNUAL FLOWERS SWEET TO THE SCENT AND THE EYES STARS OF PALE RADIANCE AMONG THE MOONLIT WOODS THE SUN BECAME WARMER THE NIGHTS CLEAR AND BOMBY\n",
      "\n",
      "Actual Label:\n",
      "IN THE MEANWHILE ALSO THE BLACK GROUND WAS COVERED WITH HERBAGE AND THE GREEN BANKS INTERSPERSED WITH INNUMERABLE FLOWERS SWEET TO THE SCENT AND THE EYES STARS OF PALE RADIANCE AMONG THE MOONLIGHT WOODS THE SUN BECAME WARMER THE NIGHTS CLEAR AND BALMY\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Take one sample from noisy validation set\n",
    "sample = noisy_eval[0]\n",
    "\n",
    "model = noisy_trainer.model\n",
    "model.eval()\n",
    "\n",
    "# Prepare input tensor\n",
    "input_tensor = torch.tensor(sample[\"input_values\"]).unsqueeze(0).to(model.device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tensor).logits\n",
    "\n",
    "# Get predicted token IDs\n",
    "pred_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# Decode prediction and label\n",
    "prediction = processor.batch_decode(pred_ids, group_tokens=True)[0]\n",
    "label = processor.batch_decode(\n",
    "    [sample[\"labels\"]],\n",
    "    group_tokens=False\n",
    ")[0]\n",
    "\n",
    "print(\"NOISY MODEL Prediction:\")\n",
    "print(prediction)\n",
    "print(\"\\nActual Label:\")\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "514c4326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean model: {'eval_loss': 14.134264945983887, 'eval_wer': 0.02054356049583891, 'eval_cer': 0.00507558566558139, 'eval_runtime': 51.3497, 'eval_samples_per_second': 9.737, 'eval_steps_per_second': 2.434, 'epoch': 5.0}\n",
      "Noisy model: {'eval_loss': 62.06174087524414, 'eval_wer': 0.051133786848072564, 'eval_cer': 0.019940403080242232, 'eval_runtime': 51.5948, 'eval_samples_per_second': 9.691, 'eval_steps_per_second': 2.423, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Clean model:\", clean_results)\n",
    "print(\"Noisy model:\", noisy_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
